5.4 Stacking

本节课ppt：https://c.d2l.ai/stanford-cs329p/_static/pdfs/cs329p_slides_7_4.pdf



刷榜利器 Stacking



![alt](https://i0.hdslb.com/bfs/note/5373029bfc7319f6f8404acafd0df109ccc315d2.png@1192w.avif)

00:06

stacking 跟之前个begging有一点像的，都是将多个base learner放在一起，来降低方差；
但是不同在于stacking的base learner可以有不同的模型类别；stacking的输出是concat起来的（模型种类不一样输出可能不同），然后再做线性的组合就得到最终的输出【begging是在数据上多次训练同样的模型；begging是所有模型的输出做平均或做投票】；
每个模型是在原始数据上训练不需要再通过bootstrap 的采样（如果base learner是随机森林的话是已经做了的）
竞赛中经常被使用，一般是每一个人训练不一样的模型，甚至是做不一样的特征提取，最后再将结果加起来


==========================================================================



stacking的效果：
![alt](https://i0.hdslb.com/bfs/note/d376a93ecfbe98b091e2b63ded8fd255ba54dd20.png@1192w.avif)


stacking中每个模型的表现：
![alt](https://i0.hdslb.com/bfs/note/d132cb7d6171cf728017c7aa3f481b97a16bf1dd.png@912w_526h.avif)

04:08

直接使用的是AutoGluon的实现

做stacking的时候没有办法保证每一个模型的效果都很好，很多时候需要加入能够提升模型精度的模型



==========================================================================



多层stacking（降低偏差）
![alt](https://i0.hdslb.com/bfs/note/c69798864a1ee5560a4b81e60f3bbda1bd9114a3.png@1192w.avif)

07:14

将第一层stacking的输出看成是特征送入下一层的stacking中（第二层的输入是基于上一层的输出，离最后的标号很近了，所以下一层训练起来会比较容易），这样可以把没有训练好的地方重新训练；
在这里也会将原始的特征直接与第一层的输出一起送入下一层
多层stacking很容易过拟合（同一份数据又送进去学了一遍）


==========================================================================



减轻多层stacking的过拟合：

![alt](https://i0.hdslb.com/bfs/note/064c9466e7ae5c554ed0b2f0655721d53fe63336.png@1192w.avif)
10:28

最简单的一种做法就是每一层的数据不是同一份数据（将数据集分成两块A、B（2层stacking），然后用A训练第一层模型L1，用L1对B进行预测，把预测的结果加上B本身训练第二层模型L2）；
每一次只用了一半的数据比较亏，所以使用重复k折bagging的方法，11:25
像k折交叉验证一样，将数据集分成k份，每一次在k-1份上训练，再用第k份做验证，这样可以训练k个模型
每个模型用这个方法训练了一次 所以每次训练了是k*n个小模型
更加昂贵的做法，对上述的做法重复做n次k折交叉验证，这样可以拿到n个预测值，再取平均放入下一层中


==========================================================================



使用多层stacking的效果：
![alt](https://i0.hdslb.com/bfs/note/779b310ae290044a051175bae44785f146497508.png@1192w.avif)

14:25



==========================================================================



stacking的总结：
![alt](https://i0.hdslb.com/bfs/note/614f036b454ad377e7d64d590df44c5703dc32f6.png@1192w.avif)

17:21

stacking就是把多个不同的模型合并起来，用来降低方差，获取多样性；
多层stacking能够把多层的模型合并起来，每一层在下一层的基础上，再更好的拟合数据，来降低偏差，但是这样的模型很容易过拟合，需要做大量的工作避免其过拟合（重复的k折bagging，每一层都能在完整的数据集上训练；重复与k折使得训练的数据都不会更上一层的数据混在一起）
stacking经常在竞赛中被使用，但问题很明显，它需要把多个模型放在一起，相对来说比较贵；关注精度和结果的可以使用它；如果觉得模型太贵了，线上表现太差可以使用蒸馏（可以使得模型变小，但是也尽量保护住了精度）


==========================================================================



最后的总结：
![alt](https://i0.hdslb.com/bfs/note/74effb3b208d436cb99d066b56df247bec9a91d9.png@1192w.avif)

19:02

集成学习就是将多个模型组合起来来降低偏差和方差
bagging是n个同样的模型做平均，主要目的是降低方差；
boosting就是几个模型组合起来，主要是降低偏差
stacking就是将多个不同的模型组合起来，降低方差
多层stacking，就是用来降低方差；
如果模型有n个的话，bagging,boosting,stacking的计算花销为之前的n倍；
bagging与stacking的好处在于每个模型基本上可以独立参与训练，所以它们是可以做并行的；boosting是训练好一个模型后再训练一个模型
最后k折多层stacking基本上可以讲上面的好处都可以拿到，但是代价也非常明显，假设有 l层 n个模型 做 k折交叉验证 则计算花销是 n*l*k ，如果还要重复多次可以还要翻几倍
虽然k折多层staking每一层是顺序的关系，但是一层的里面是并行的