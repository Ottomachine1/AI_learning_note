5.3 Boosting

本节课ppt：https://c.d2l.ai/stanford-cs329p/_static/pdfs/cs329p_slides_7_3.pdf



Boosting
![alt](https://i0.hdslb.com/bfs/note/b01da9e7a2e6508b95f865c7d2f344f8d158b365.png@1192w.avif)

00:06

boosting 它是说将多个弱一点的模型（偏差比较大）组合起来变成强一点的模型（偏差比较小），主要是为了去降低偏差而不是方差【Bagging 把多个不那么稳定的模型把它们放在一起得到一个相对稳定的模型】
Boosting是要按顺序的学习【bagging是每个模型是独立的】
具体的做法：每一次在第i步的时候，会训练一个弱模型h i，然后去评估一下它的误差εt；然后根据当前的误差εt重新采样一下，使得接下来的模型h i+1会去关注那些预测不正确的样本；
比较著名的样例：AdaBoost,Gradient boosting


==========================================================================



Gradient boosting

![alt](https://i0.hdslb.com/bfs/note/ca3af0418881f07836cefa6c79a9e0c93a7400b0.png@1192w.avif)
02:22

假设在时间 t 时训练好的模型时Ht(x)，时间1时H1(x)=0

在残差上训练新的模型ht【残差指在m个样本中样本本身 特征不发生变化，标号变了 实际标号减去预测的标号（也就是后面说的拟合不够的差值）】
 时间1时 是在原始的样本上进行训练，但是在之后的时间内，都是把当前时刻boosting的模型去拟合 拟合不够的那个差值
然后将ht * η（学习率）加进当前整个boosting出来的模型，变成下一个时刻boosting出来的模型【η是作为一个正则项，boosting中叫收缩】（当然η可以为1，但是这样很容易过拟合）


残差实际上等价于 MSE作为损失函数时 损失函数L对函数H做负梯度



为什么会讲gradient boosting？是因为其他的boosting 函数可以换到gradient boosting的框架里面（取决于要怎么去选损失函数L）



==========================================================================



gradient boosting 的代码实现


07:38
![alt](https://i0.hdslb.com/bfs/note/5bb42caf9e480f1260112052a98f6ba84595a9f3.png@1192w.avif)


==========================================================================



gradient boosting的效果

![alt](https://i0.hdslb.com/bfs/note/4e85daff6a170c2597518d456354e0f01e8dcf3b.png@1192w.avif)
09:34

用决策树作为weak learner（Gradient boosting 很容易 过拟合，所以我们需要对它做些正则化，需要用一个弱的模型来做）
但是决策树是一个强模型，我们需要限制树的最高层数，也可以随机采样一些特征（列）
在GBDT中模型没有过拟合（每一个小模型确实比较弱，学习率也定的比较低），当weak learner、学习率 控制得比较好的时候，它的过拟合现象没那么严重
GBDT需要顺序训练，在大的训练集上会比较吃亏，所以会用一些加速算法如：XGBoost，lightGBM


==========================================================================



总结：


13:49

Boosting是说把n个弱一点的模型组合在一起变成一个比较强的模型，用于降低偏差
Gradient boosting 是boosting的一种，每一次弱的模型是去拟合 在标号上的残差，可以认为是每次去拟合给定损失函数的负梯度方向