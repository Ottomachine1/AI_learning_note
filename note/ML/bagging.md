## 5.2 Bagging

本节课ppt:https://c.d2l.ai/stanford-cs329p/_static/pdfs/cs329p_slides_7_2.pdf

bagging（来自Bootstrap AGGrgratING）

![alt](https://i0.hdslb.com/bfs/note/790e7be5661c5822f9369638160024fbb46a58c5.png@1192w.avif)
00:05

做bagging的时候，每次训练n个模型（base learners），但是每个模型都是独立并行训练的，在得到n个模型之后，
如果是回归问题，会把每一个模型的输出做平均就得到了bagging出来的效果
如果是做分类的话，这样每一个模型都会输出一个类别，然后会用这些输出做投票选最多的（这个叫Majority voting）
每个模型的训练是通过bootstrap采样得到的训练集上训练的
什么是bootstrap采样？假设训练集有m个样本，每一次训练base learner的时候，随机采样m个样本，每次采样我们会将这个样本放回去，可能有些样本会重复，如果有n个模型要这样训练，就重复n次
大概是有1-1/e ≈63%的概率会被采样到，就是说可能有37%的样本是没有采样出来的，可以用这个来做验证集，这个也叫做 out of bag


P.S.1-1/e的来源可以看西瓜书的p27页上面有说



代码实现：

![alt](https://i0.hdslb.com/bfs/note/92de7426ccddb1f460251fdf6b21c0900948fec1.png@1192w.avif)
04:20



==========================================================================



## 随机森林


07:16
![alt](https://i0.hdslb.com/bfs/note/606e9b971c37c627a50e6ed34a9c5645c347e3ed.png@1192w.avif)
随机森林使用决策树来做base learner；
使用随机森林时的常用技术，在bootstrap样本时还会每次随机采样一些特征出来，但在这个地方就不会去采样重复的类出来，因为重复的类没有太大的意义；这样做主要的好处是随机采样之后可以避免一定的过拟合，而且能够增加每一棵决策树之间的差异性；
在右边的曲线图中，我们可以知道，随着learner的数量增加，模型的误差是逐渐减小的。但是泛化误差的曲线不会往上升，这是因为我们降低了方差但没使得偏差更大，这也就改善了泛化误差中三项其中的一项，但没增加另外两项


==========================================================================



## bagging什么时候会变好


10:05
![alt](https://i0.hdslb.com/bfs/note/fd5b631b11fee64b9606109dd684a0a14b0c239f.png@1192w.avif)
bagging主要下降的是方差，在统计上采样1次和采样n次取平均，它的均值是不会发生变化的就bias是不会发生变化的，唯一下降的是方差，采样的越多，方差相对来说变得越小。
方差什么时候下降的比较快，方差比较大的时候下降的相关比较好。
那什么时候方差大呢，方差比较大的模型我们叫做unstable的模型；以回归来举例子，真实的是f ，base learner是h，bagging之后 对每个学到的base learner的预测值取个均值 就会得到预测值f_hat；因为期望的平方会小于方差，所以h(x)与f(x)差别很大的时候，bagging的效果比较好
也就是说，在base learner没那么稳定的时候，它对于下降方差的效果会好




==========================================================================



## 不稳定的learner：


14:38
![alt](https://i0.hdslb.com/bfs/note/1312c6dd5b08733c22de752100c4620ae83ab826.png@1192w.avif)
决策树不是一个稳定的learner，因为数据一旦发生变化，选取的特征然后选取特征的哪个值都会不一样，分支会不一样，故其不稳定；
线性回归比较稳定，数据的较小的变化，对模型不会有太大的影响


==========================================================================



## 总结：


16:08

bagging就是训练多个模型，每个模型就是通过在训练数据中通过bootstrap采样训练而来；
bootstrap就是每一次用m个样本，随机在训练数据中采样m个样本，且会放回继续采样
bagging的主要效果是能够降低方差，特别是当整个用来做bagging的模型是不稳定的模型的时候效果最佳（随机森林）
