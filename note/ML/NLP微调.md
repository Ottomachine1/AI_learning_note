11.2 NLP中的微调



本节课ppt：https://c.d2l.ai/stanford-cs329p/_static/pdfs/cs329p_slides_14_2.pdf

 

==========================================================================



自监督预训练：

![alt](https://i0.hdslb.com/bfs/note/95260069e1a210d99a3f631e0402c2eb83df6f1b.png@1192w.avif)
00:13

NLP与CV在微调中有个不一样的地方是说：NLP里面不存在特别大的标好的数据集（CV中图片分类里，有使用特别大的标注好的数据集训练好的预训练模型），在NLP中机器翻译里面有大一点的，但是在NLP中更多是说有大量的没有标注的文档（如Wikipedia、网络的电子书、爬取网站中的文字等）；
在NLP里面我们很容易得到大量的文档，但是比较难的是去给这些文档一个个去做标号。在早期，我们可以去分类这些文档，但是标注是非常非常难的。
在NLP中，我们一般会去做自监督的一个预训练（标号是由自己产生的，也就是伪标号。然后用这些伪标号来参与进 一个正常的可以监督学习的任务之中，用于完成预训练）
在NLP中有两个常见的应用来生成这样的伪标号
语言模型（Language Model，LM）：给出前面的一些词，去预测下一个词；
带掩码的语言模型（Masked Language Model，MLM）：在一个句子里面随机的扣掉一些字，然后让你去预测这个字，有点像是完型填空；
完型填空比预测下一个字容易些，因为完型填空能看到左右的信息，而预测比较开放；


==========================================================================



NLP中的预训练模型：


02:40

词嵌入（十几年前的模型）：模型会学习两个向量u_w和v_w，然后对给定掩码的词Y，用其x1……xn去预测Y
具体来说，要预测的值用u来表示，上下文的词用v来表示，然后把v给加起来再与u做内积，这样就可以的到u与v之间的关系，这个算法叫CBOW；然后做预测时就从字典中选取一个y使得CBOW的值最大；
当然，也可以用中心词去预测周围的词。具体是说，对所有的词学到了两个向量，这两个词具有一定的语义关系（两个词如果差不多同时出现的话，那么存在相似度），如果说y与xi出现在一起的话，内积比较大的话，意味着说有相似性；
基于Transformer的预训练模型（BERT）：
06:07

BERT就是一个Transformer的编码器，然后它使用的是带掩码的词预测（双向的），所以它适合一些带掩码的语言模型；
GPT用的是它的一个解码器（从左到右的过程），可以用于预测下一个词；
T5是一个基于编码器与解码器的架构，


==========================================================================



稍微讲讲BERT：


06:44

论文精讲BERT：https://www.bilibili.com/video/BV1PL411M7eQ
BERT使用了两个预训练任务：一个是把一个词盖住去预测，另一个是为了处理NLP里多的任务，一个进去两个句子（可以是文档中确实是相连的（正例），没有相连的句子（负例））；
BERT原始的论文使用的是Wikipedia和一个书的数据集；
有许多的版本：Base/Large（大小不一样）；English/Multilingual（在英语上训练还是在英语上训练的）；cased/uncase（有没有大小写的区别）；
看图看BERT在干什么事情
07:49

BERT之后有很多变种，ALBERT，ELECTRA，RoBERTa；


==========================================================================



BERT的微调（Fine-Tuning）:


09:11

在训练好上面的模型之后就可以用BERT做fine-tuning了；
和CV很像，构造我们的任务的最后一层让它输出到我们想要的东西（BERT的最后一层要去掉，因为最后一层是输出两句子间是否相连），也就是最后一层随机初始化，训练一些epoch，用一个比较小的学习率；
当然这也取决于下游的任务：
对句子分类（判断句子的正面与负面）：只需要把第一个词对应输出拿出来，然后加一个输出层就可以拿到标号了；
做词级别的任务（实体命名（人名街道名之类的）的识别）：把整个那些词，每个词对应的输出的向量拿过去接上个全连接层做标号就行了；
做问答的话（给一个问题或给一段话，答案就在这一段话里面，模型需要把答案给找出来（给出答案的开始与结束的位置）），对第二个句子开始就要每个词的输出里面去看他是不是答案的开始，是不是答案的结束，
虽然在自然语言中任务有多种多样，但实际上可以通过构造输入（句子、词、一段话、甚至可以是一篇文章）使得能放入进模型里面


==========================================================================



实际中要考虑的东西：


12:37

BERT这篇paper出来的时候，微调时结果不稳定（超参数没有弄对），因为BERT使用了Adam的改版（前期的时候去掉了整个对系数估计的不确定性），这个对BERT是没问题的（数据量很大）；但是对于微调来讲，它的数据集不大也不会跑很久，再使用改版的Adam会使得结果没那么稳定；所以推荐使用完整版的Adam；
BERT对哪个任务都是训练3次数据迭代，大家发现三次并没有完整的收敛，最好是训练多几个Epoch；
这些是比较常见的小技巧，做任何微调时都可以去调一下这些东西；
对于一个Transformer来讲，它跟一个CNN是没有什么本质上的不同的，就是有很多层，对底层来说是有保留一些更低的语义层次的信息，越往上的话它跟标注空间越像；在BERT中也可以像CV一样固定住下面的层只调上面的，甚至可以对上面那些层进行随机初始化（不一定要用预训练里训练好的权重）【可以不受之前语言模型的影响】




==========================================================================





去哪里找NPL中的预训练模型：


15:15

在做Transformer这一块的话，常见的一个库叫HuggingFace（其实是一个公司的名字，他们做了一个包(transformers)，提供了大量的Pytorch、Tensorflow的实现），现在社区庞大，所以就包含了可以做模型的预训练，也包含了很多的新模型；
简单例子：

16:07



==========================================================================





应用：


18:45

BERT在11门语言上取得了最好的结果：包含了去判断一个句子中的词的语法是否正确，判断评论的正负与否，判断两句话在语义上是否等价，或者两个问题在语义上是否等价，判断假设和结论是否有加强关系，在问题里面能不能找到答案；
BERT采用的是编码器的架构，会有局限性，在适合编码器解码器架构的任务就不会那么适用了；
GPT更适合给出一段话，然后生成更长的一段话；
T5也在很多地方取得了不错的结果，它使用的是编码器解码器的架构，可以做一些别的不一样的事情（对文章做摘要，或者是做Q&A）
根据自己的任务来选择使用Transformer的哪个部分（BERT，GPT，T5等）


==========================================================================



总结：


20:39

在自然语义里面，预训练模型通常是通过自监督来完成的（因为自然语言的数据集没有那么大的标好的数据集），一个常见的应用是做语言模型或者是做带掩码的语言模型；
BERT是一个特别大的Transformer的编码器，GPT就是一个特别大的解码器，T5就是一个特别大的解码器加编码器；
BERT在对下游任务做微调时就跟CV很像了，一般只要改最后一层就好了，但是在输入上要有一定的构造；
