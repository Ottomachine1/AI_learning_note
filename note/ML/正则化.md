10.1 深度神经网络架构

本节课ppt：https://c.d2l.ai/stanford-cs329p/_static/pdfs/cs329p_slides_13_1.pdf



这一章主要是关于深度神经网络里面的一些共用的设计模式：

批量与层的归一化；
残差连接（PPT：https://c.d2l.ai/stanford-cs329p/_static/pdfs/cs329p_slides_13_2.pdf）；
注意力机制（PPT：https://c.d2l.ai/stanford-cs329p/_static/pdfs/cs329p_slides_13_3.pdf）；
本节课讲批量与层的归一化（另外两个论文精读有讲）；



论文精读链接：

resNet：https://www.bilibili.com/video/BV1P3411y7nn

Attention：https://www.bilibili.com/video/BV1pu411o7BE



========================================================================== 



深度神经网络的设计

![alt](https://i0.hdslb.com/bfs/note/e21230a960c1ef411155d235a214d245091361f6.png@1192w.avif)
00:06

深度学习(DL)是一个编程语言，用这个语言来表达我们对数据的一些理解（数据结构上的，设计上，理念上的）；
与一般编程语言不一样的是说，深度神经网络里面有很多值（可以学的权重），具体的值会根据真实的数据得到【就相当于我们定义了一个模板，模板里面的东西由之后的学习中获取到】；
整个学习过程是可以导的，然后在给了损失函数和数据之后，就可以通过误差反向传递来进行对权重的更新。
它跟一般的语言一样，也是有很多的设计模式（层是怎么设计的；整个网络是怎么设计的）


========================================================================== 
![alt](https://i0.hdslb.com/bfs/note/e21230a960c1ef411155d235a214d245091361f6.png@1192w.avif)


批量归一化


01:48

在线性模型中，我们会对数据进行标准化（使得数据的每一个特征均值为0方差为1），从数学上来说，这样能使得损失函数更加平滑（特别是对于线性模型来说）
平滑是指 损失函数对x的导数 与 损失对y的导数 之间的差的平方和，会小于等于x与y之间的差的平方和 的β倍。这意味的是说 x走的很远的时候，它的梯度变化得不是很大 （我走了很远但是方向没有走得太偏）
02:49

因此，β比较小得时候，可以有一个比较大得学习率（学习率等价于步长）；
但是标准化不会帮助深度神经网络，因为如果是对x做标准化的话，他只会帮助直接线性作用于x上的那个函数，也就是线性模型是可以的。但在多层的情况下，他会帮助最下面那层的线性层；
批量归一化就是说，把中间那些层的输入也做标准化，这样能帮助整个函数更加平滑，使得在训练深度神经网络的时候会更加容易（这个观点还是争议的）；
使用批量归一化之后，收敛上会更加容易，可以选用更大的一些学习率，但是一般来说不会改变最后的结果（精度跟没有差不多，但是可以快一点）


========================================================================== 



具体看看 批量归一化
![alt](https://i0.hdslb.com/bfs/note/905f824dc1582527c668cc83fe7ceee225d68e80.png@1192w.avif)

06:35

批量归一化可以拆解为四步：

变形（Reshape）：如果输入是2维的矩阵就不用改变，不是的话就要改成2维的【举个例子，假设输入是个卷积（一般是四维的，n（批量大小维）、c（RGB 通道或卷积的输出通道）、w（宽）、h（高）），我们会在这一步将其变化为2维的矩阵，由n * c * w * h变为nwh * c ，就是把通道维拉到最后，把nwh这三个维度合并在一起，可以这么理解 c 在CNN中表示的是一个识别出特征，而nwh则是样本的数据】
标准化（Normalization）：具体来说是对每一列标准化，也就是对变形后的矩阵的一列 减去 这一列的均值 再除以这一列的方差，；
还原（Recover）：用我们标准化后的矩阵y 对它的一列乘上 这一列对应的γ 加上这一列对应的β，在这个地方是说虽然我们对数据减了均值后再除了方差，但是我们还是有点想要数据有一点偏差，那么这个步骤就允许这个还原回去【如果γ为方差，β为均值，那么将会还原回去】，在这里γ与β是可以学习的， 神经网络会根据需求去找谁会更好一点；
最后就是将处理后的变形矩阵给变回去


批量归一化的代码实现

![alt](https://i0.hdslb.com/bfs/note/dfa6383a820ac6da3950b0c8038ad80c19d8103a.png@1192w.avif)

11:26

完整代码：http://d2l.ai/chapter_convolutional-modern/batch-norm.html

动手学深度学习的讲解：https://www.bilibili.com/video/BV1X44y1r77r



========================================================================== 



层归一化（Layer Normalization）：
![alt](https://i0.hdslb.com/bfs/note/8583c0c805acbebd4813deaa2fecc9d336cec282.png@1192w.avif)

18:38

层归一化主要是用在循环神经网络（RNN）里面，因为将BN用于RNN中时，每一个时间步骤都得用自己得均值方差，甚至是学到得γ与β，在每一时间步中最好不要共享这些均值与方差（在不同的时间步中，这些数值变化还是很大的，而BN需要一个比较稳定的均值方差的估计，抖动比较大的均值与方差就失去了做标准化的意义）
层归一化到底是在做什么：在变形的步骤上对输入矩阵做转置（2维就普通转置，4维就把cwh放在一起再做转置），其他的步骤与BN相同；如果是RNN的输入矩阵（N * P * t）的话，就把P与t放在一起
其实这就是标准化每一个样本到当前的时间步骤，这样做的好处是说在做预测时不需要存均值方差这种全局的东西
在CNN上的效果一般，在RNN特别是在Transformer上效果非常好


========================================================================== 



别的Normalization
![alt](https://i0.hdslb.com/bfs/note/a59f466ae420a4569b8da87b484797f9da118d4e.png@1192w.avif)39

对变形这一步做修改：
InstanceNorm：也是对CNN用的，就是将 输出通道数与批量数放在一起，长与宽放在一起；
GroupNorm：就是将 输出通道数拆成 s*g 然后s与wh放在一起，g与批量数放在一起；
CrossNorm：将均值与方差在算出来之后，不是弄成0和1 而是将它两交换一下；
对标准化这一步做修改：白化（Whitening）：不仅是将均值变0方差变1，而且使得每一个特征之间是没有关系的（做一次PCA）;
对还原这一步做修改：将可学习的γ、β变为一个线性层、甚至可以把它变成MLP，来还原
还有一些变种是说将其作用在层的输入上，也可以对权重或是梯度做标准化
不同的normalization 就有不同的适用性（在GAN中的 BN 效果不是很好，在Adversarial Attack 中用BN也不是很好），所以要选取适合的normalization技术



========================================================================== 



总结：

![alt](https://i0.hdslb.com/bfs/note/d2bf5dfa486673d5d579eb942ed704c79823156c.png@1192w.avif)
27:52

Normalization就是把一些中间的层的在数值上变得稳定点，让整个损失函数更加平滑，可以使得神经网络的训练更加容易，一般来说不会改变最后的精度；
不用normalization的话，很多时候也没有关系的，可以通过其他方式来弥补（关键是看怎么让损失函数变得平滑点）
一个normalization的layer就是将输入换成合适的形式然后把他的列进行归一化，再做一次还原步骤（如果想要一个不一样方差不一样的均值，这里的参数是可以学习的），最后就换回原来的形状
比较著名的例子是：在CNN中BN用的多一点，LN在transformer用的多一点。